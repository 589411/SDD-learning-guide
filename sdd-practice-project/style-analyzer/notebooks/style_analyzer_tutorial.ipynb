{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¨ é¢¨æ ¼ç‰¹å¾µæå–å™¨ - Google Colab æ•™å­¸\n",
    "\n",
    "> **é€™æ˜¯ä¸€å€‹è¦æ ¼é©…å‹•çš„å°ˆæ¡ˆ**  \n",
    "> ä½ å¯ä»¥ä½¿ç”¨ `specs/style-analyzer.spec.md` ä¸­çš„è¦æ ¼ï¼Œé…åˆ AI å·¥å…·å®Œæˆé–‹ç™¼ã€‚\n",
    "\n",
    "## ğŸ“‹ å­¸ç¿’ç›®æ¨™\n",
    "\n",
    "1. ç†è§£è¦æ ¼é©…å‹•é–‹ç™¼ï¼ˆSDDï¼‰\n",
    "2. ä½¿ç”¨ AI å·¥å…·ï¼ˆGeminiï¼‰è¼”åŠ©é–‹ç™¼\n",
    "3. å¯¦ä½œæ–‡å­—é¢¨æ ¼åˆ†æåŠŸèƒ½\n",
    "4. é«”é©—æ¸¬è©¦é©…å‹•é–‹ç™¼ï¼ˆTDDï¼‰\n",
    "\n",
    "## ğŸš€ å¿«é€Ÿé–‹å§‹\n",
    "\n",
    "é»æ“Šä¸Šæ–¹çš„ã€Œåœ¨ Colab ä¸­é–‹å•Ÿã€æŒ‰éˆ•ï¼Œæˆ–ç›´æ¥åŸ·è¡Œä»¥ä¸‹å–®å…ƒæ ¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1ï¼šå®‰è£ä¾è³´å¥—ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk vaderSentiment pytest -q\n",
    "\n",
    "# ä¸‹è¼‰ NLTK è³‡æ–™\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"âœ… ä¾è³´å¥—ä»¶å®‰è£å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2ï¼šæŸ¥çœ‹è¦æ ¼æ–‡ä»¶\n",
    "\n",
    "é€™å€‹å°ˆæ¡ˆçš„å®Œæ•´è¦æ ¼åœ¨ `specs/style-analyzer.spec.md`ã€‚\n",
    "\n",
    "**æ ¸å¿ƒåŠŸèƒ½ï¼š**\n",
    "1. æƒ…æ„Ÿåˆ†æï¼šè¨ˆç®—ç†±æƒ…åº¦å¾—åˆ† (0.0-1.0)\n",
    "2. å¥å­çµ±è¨ˆï¼šå¹³å‡/æœ€é•·/æœ€çŸ­å¥å­é•·åº¦\n",
    "3. é«˜é »çŸ­èªï¼šæå– TOP 3 å¸¸ç”¨çŸ­èª\n",
    "4. å•å¥æ¯”ä¾‹ï¼šè¨ˆç®—å•å¥ä½”æ¯”\n",
    "\n",
    "**é©—æ”¶æ¢ä»¶ç¯„ä¾‹ï¼š**\n",
    "```gherkin\n",
    "Given æˆ‘æœ‰ä¸€ä»½æ–‡å­—æª”æ¡ˆ \"article.txt\"\n",
    "  And æª”æ¡ˆå…§å®¹è‡³å°‘æœ‰ 500 å­—\n",
    "When æˆ‘ä¸Šå‚³æª”æ¡ˆåˆ°åˆ†æå™¨\n",
    "Then ç³»çµ±æ‡‰è©²å›å‚³ JSON æ ¼å¼çš„åˆ†æå ±å‘Š\n",
    "  And å ±å‘Šæ‡‰è©²åŒ…å«ã€Œç†±æƒ…åº¦å¾—åˆ†ã€\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3ï¼šå¯¦ä½œé¢¨æ ¼åˆ†æå™¨\n",
    "\n",
    "### æ–¹æ³• Aï¼šä½¿ç”¨ AI è¼”åŠ©ï¼ˆæ¨è–¦ï¼‰\n",
    "\n",
    "1. é–‹å•Ÿ Google AI Studio: https://aistudio.google.com/\n",
    "2. è¤‡è£½è¦æ ¼æ–‡ä»¶ä¸­çš„ã€Œå®Œæ•´è¦æ ¼è¤‡è£½å€ã€\n",
    "3. è²¼çµ¦ Geminiï¼Œè®“å®ƒç”Ÿæˆç¨‹å¼ç¢¼\n",
    "4. å°‡ç”Ÿæˆçš„ç¨‹å¼ç¢¼è²¼åˆ°ä¸‹æ–¹\n",
    "\n",
    "### æ–¹æ³• Bï¼šæ‰‹å‹•å¯¦ä½œ\n",
    "\n",
    "æ ¹æ“šè¦æ ¼é€æ­¥å¯¦ä½œæ¯å€‹åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¢¨æ ¼åˆ†æå™¨é¡åˆ¥\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "\n",
    "class StyleAnalyzer:\n",
    "    \"\"\"æ–‡å­—é¢¨æ ¼åˆ†æå™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_formats = ['.txt', '.md']\n",
    "        self.min_length = 100\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    def analyze_text(self, text: str) -> dict:\n",
    "        \"\"\"åˆ†ææ–‡å­—å…§å®¹\"\"\"\n",
    "        # åŸºæœ¬çµ±è¨ˆ\n",
    "        word_count = len(text)\n",
    "        sentences = self._split_sentences(text)\n",
    "        sentence_count = len(sentences)\n",
    "        \n",
    "        # æƒ…æ„Ÿåˆ†æ\n",
    "        sentiment_scores = self.sentiment_analyzer.polarity_scores(text)\n",
    "        enthusiasm_score = (sentiment_scores['compound'] + 1) / 2  # è½‰æ›ç‚º 0-1\n",
    "        \n",
    "        # å¥å­é•·åº¦çµ±è¨ˆ\n",
    "        sentence_lengths = [len(s) for s in sentences if s.strip()]\n",
    "        avg_length = sum(sentence_lengths) / len(sentence_lengths) if sentence_lengths else 0\n",
    "        \n",
    "        # å•å¥æ¯”ä¾‹\n",
    "        question_count = text.count('ï¼Ÿ') + text.count('?')\n",
    "        question_ratio = question_count / sentence_count if sentence_count > 0 else 0\n",
    "        \n",
    "        # è­¦å‘Š\n",
    "        warnings = []\n",
    "        if word_count < 500:\n",
    "            warnings.append(f\"å…§å®¹éçŸ­ï¼ˆ{word_count}å­—ï¼‰ï¼Œå»ºè­°è‡³å°‘ 500 å­—\")\n",
    "        \n",
    "        return {\n",
    "            \"analysis_date\": datetime.now().isoformat(),\n",
    "            \"word_count\": word_count,\n",
    "            \"sentence_count\": sentence_count,\n",
    "            \"metrics\": {\n",
    "                \"enthusiasm_score\": round(enthusiasm_score, 2),\n",
    "                \"avg_sentence_length\": round(avg_length, 1),\n",
    "                \"max_sentence_length\": max(sentence_lengths) if sentence_lengths else 0,\n",
    "                \"min_sentence_length\": min(sentence_lengths) if sentence_lengths else 0,\n",
    "                \"question_ratio\": round(question_ratio, 2),\n",
    "                \"top_phrases\": []  # TODO: å¯¦ä½œé«˜é »çŸ­èªæå–\n",
    "            },\n",
    "            \"warnings\": warnings\n",
    "        }\n",
    "    \n",
    "    def _split_sentences(self, text: str) -> list:\n",
    "        \"\"\"åˆ‡åˆ†å¥å­\"\"\"\n",
    "        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\\.\\!\\?]', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "print(\"âœ… StyleAnalyzer é¡åˆ¥å®šç¾©å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4ï¼šæ¸¬è©¦åˆ†æå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹åˆ†æå™¨å¯¦ä¾‹\n",
    "analyzer = StyleAnalyzer()\n",
    "\n",
    "# ç¯„ä¾‹æ–‡å­—\n",
    "sample_text = \"\"\"\n",
    "AI æ™‚ä»£çš„è»Ÿé«”é–‹ç™¼æ–°å…¸ç¯„\n",
    "\n",
    "å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘æƒ³å’Œå¤§å®¶åˆ†äº«ä¸€å€‹ä»¤äººèˆˆå¥®çš„ä¸»é¡Œï¼šAI å¦‚ä½•æ”¹è®Šè»Ÿé«”é–‹ç™¼çš„æ–¹å¼ã€‚\n",
    "\n",
    "éå»ï¼Œæˆ‘å€‘èŠ±å¤§é‡æ™‚é–“åœ¨å¯«ç¨‹å¼ç¢¼ã€é™¤éŒ¯ã€æ¸¬è©¦ã€‚ä½†ç¾åœ¨ï¼ŒAI å·¥å…·å¦‚ ChatGPTã€GitHub Copilot è®“é–‹ç™¼é€Ÿåº¦æå‡äº† 10 å€ï¼é€™çœŸçš„å¤ªæ£’äº†ï¼\n",
    "\n",
    "å…·é«”ä¾†èªªï¼Œè¦æ ¼é©…å‹•é–‹ç™¼ï¼ˆSDDï¼‰åœ¨ AI æ™‚ä»£è®Šå¾—æ›´åŠ é‡è¦ã€‚ç‚ºä»€éº¼å‘¢ï¼Ÿå› ç‚º AI éœ€è¦æ¸…æ¥šçš„æŒ‡ä»¤æ‰èƒ½ç”¢å‡ºé«˜å“è³ªçš„ç¨‹å¼ç¢¼ã€‚æ›å¥è©±èªªï¼Œä½ çš„è¦æ ¼å¯«å¾—è¶Šæ¸…æ¥šï¼ŒAI çš„ç”¢å‡ºå°±è¶Šæº–ç¢ºã€‚\n",
    "\n",
    "è®“æˆ‘èˆ‰å€‹å¯¦éš›æ¡ˆä¾‹ï¼šç•¶æˆ‘ç”¨ Cursor é–‹ç™¼æ™‚ï¼Œæˆ‘æœƒå…ˆå¯«å¥½å®Œæ•´çš„è¦æ ¼ï¼Œç„¶å¾Œè®“ AI æ ¹æ“šè¦æ ¼ç”Ÿæˆç¨‹å¼ç¢¼ã€‚çµæœå‘¢ï¼Ÿæ¸¬è©¦ä¸€æ¬¡å°±é€šéï¼é€™åœ¨ä»¥å‰æ˜¯ä¸å¯èƒ½çš„ã€‚\n",
    "\n",
    "é‚£éº¼ï¼Œæˆ‘å€‘æ‡‰è©²å¦‚ä½•é©æ‡‰é€™å€‹æ–°æ™‚ä»£ï¼Ÿæˆ‘çš„å»ºè­°æ˜¯ï¼š\n",
    "1. å­¸ç¿’æ’°å¯«æ¸…æ™°çš„è¦æ ¼\n",
    "2. æŒæ¡ AI å·¥å…·çš„ä½¿ç”¨\n",
    "3. åŸ¹é¤Šé©—æ”¶å’Œåˆ¤æ–·çš„èƒ½åŠ›\n",
    "\n",
    "ç¸½ä¹‹ï¼ŒAI æ™‚ä»£çš„é–‹ç™¼è€…ä¸æ˜¯è¢«å–ä»£ï¼Œè€Œæ˜¯è¢«è³¦èƒ½ã€‚é—œéµåœ¨æ–¼ä½ æ˜¯å¦é¡˜æ„å­¸ç¿’æ–°çš„å”ä½œæ–¹å¼ã€‚è®“æˆ‘å€‘ä¸€èµ·æ“æŠ±é€™å€‹å……æ»¿å¯èƒ½æ€§çš„æœªä¾†å§ï¼\n",
    "\"\"\"\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "result = analyzer.analyze_text(sample_text)\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "print(\"ğŸ“Š åˆ†æçµæœï¼š\")\n",
    "print(json.dumps(result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5ï¼šè¦–è¦ºåŒ–çµæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# è¨­å®šä¸­æ–‡å­—é«”\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'DejaVu Sans']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# å»ºç«‹åœ–è¡¨\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# åœ–è¡¨ 1ï¼šç†±æƒ…åº¦å¾—åˆ†\n",
    "enthusiasm = result['metrics']['enthusiasm_score']\n",
    "axes[0].barh(['Enthusiasm Score'], [enthusiasm], color='#4CAF50')\n",
    "axes[0].set_xlim(0, 1)\n",
    "axes[0].set_title('Enthusiasm Score', fontsize=14, fontweight='bold')\n",
    "axes[0].text(enthusiasm + 0.05, 0, f'{enthusiasm:.2f}', va='center')\n",
    "\n",
    "# åœ–è¡¨ 2ï¼šå¥å­é•·åº¦çµ±è¨ˆ\n",
    "metrics = result['metrics']\n",
    "lengths = [\n",
    "    metrics['avg_sentence_length'],\n",
    "    metrics['max_sentence_length'],\n",
    "    metrics['min_sentence_length']\n",
    "]\n",
    "labels = ['Avg', 'Max', 'Min']\n",
    "axes[1].bar(labels, lengths, color=['#2196F3', '#FF9800', '#F44336'])\n",
    "axes[1].set_title('Sentence Length', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Characters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ å­—æ•¸ï¼š{result['word_count']}\")\n",
    "print(f\"ğŸ“ å¥å­æ•¸ï¼š{result['sentence_count']}\")\n",
    "print(f\"â“ å•å¥æ¯”ä¾‹ï¼š{result['metrics']['question_ratio']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 6ï¼šä¸Šå‚³ä½ è‡ªå·±çš„æ–‡å­—æª”æ¡ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# ä¸Šå‚³æª”æ¡ˆ\n",
    "print(\"è«‹ä¸Šå‚³ä½ çš„æ–‡å­—æª”æ¡ˆï¼ˆ.txt æˆ– .mdï¼‰ï¼š\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# åˆ†æä¸Šå‚³çš„æª”æ¡ˆ\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\nåˆ†ææª”æ¡ˆï¼š{filename}\")\n",
    "    \n",
    "    # è®€å–æª”æ¡ˆ\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # åŸ·è¡Œåˆ†æ\n",
    "    result = analyzer.analyze_text(content)\n",
    "    \n",
    "    # é¡¯ç¤ºçµæœ\n",
    "    print(json.dumps(result, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ å­¸ç¿’ç¸½çµ\n",
    "\n",
    "### ä½ å­¸åˆ°äº†ä»€éº¼ï¼Ÿ\n",
    "\n",
    "1. âœ… **è¦æ ¼é©…å‹•é–‹ç™¼ï¼ˆSDDï¼‰**\n",
    "   - å…ˆå®šç¾©è¦æ ¼ï¼ˆè¦åšä»€éº¼ï¼‰\n",
    "   - å†å¯¦ä½œåŠŸèƒ½ï¼ˆæ€éº¼åšï¼‰\n",
    "   - ç”¨æ¸¬è©¦é©—è­‰ï¼ˆåšå°äº†å—ï¼‰\n",
    "\n",
    "2. âœ… **AI è¼”åŠ©é–‹ç™¼**\n",
    "   - ç”¨è¦æ ¼æŒ‡å° AI\n",
    "   - AI ç”Ÿæˆç¨‹å¼ç¢¼\n",
    "   - äººé¡é©—æ”¶çµæœ\n",
    "\n",
    "3. âœ… **å¯¦ç”¨æŠ€èƒ½**\n",
    "   - æ–‡å­—åˆ†æ\n",
    "   - æƒ…æ„Ÿåˆ†æ\n",
    "   - è³‡æ–™è¦–è¦ºåŒ–\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "1. ğŸ“ å®Œå–„åŠŸèƒ½ï¼ˆå¯¦ä½œé«˜é »çŸ­èªæå–ï¼‰\n",
    "2. ğŸ§ª å¢åŠ æ¸¬è©¦ï¼ˆç¢ºä¿å“è³ªï¼‰\n",
    "3. ğŸš€ éƒ¨ç½²æ‡‰ç”¨ï¼ˆè®“åˆ¥äººä½¿ç”¨ï¼‰\n",
    "4. ğŸ“š å˜—è©¦å…¶ä»–å°ˆæ¡ˆï¼ˆnews-to-lesson, knowledge-baseï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ­å–œå®Œæˆé¢¨æ ¼åˆ†æå™¨æ•™å­¸ï¼**\n",
    "\n",
    "æ›´å¤šè³‡æºï¼š\n",
    "- [å®Œæ•´è¦æ ¼æ–‡ä»¶](../specs/style-analyzer.spec.md)\n",
    "- [å°ˆæ¡ˆ README](../README.md)\n",
    "- [SDD å­¸ç¿’æŒ‡å—](https://github.com/ä½ çš„å¸³è™Ÿ/SDD-learning-guide-main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
